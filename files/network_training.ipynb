{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ff9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "\n",
    "import import_ipynb\n",
    "from neural_networks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor(delta, x):\n",
    "\tx[x == None] = 0\n",
    "\tdelta[delta == None] = 0\n",
    "\treturn outer(delta, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11000380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source = array([ 1.5, 2.5 ])\n",
    "target = array([ 0.427, -0.288 ])\n",
    "vector = hstack([source, target])\n",
    "vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d913462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_inputs(w): return sum(inputs(w))\n",
    "def num_outputs(w): return sum(outputs(w))\n",
    "def num_biases(w): return sum(biases(w))\n",
    "def num_neurons(w): return sum(neurons(w))\n",
    "\n",
    "num_inputs(w), num_outputs(w), num_biases(w), num_neurons(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_weight_gradient(vector, w, case = 'square'):\n",
    "\tnum_in = num_inputs(w)\n",
    "\tnum_out = num_outputs(w)\n",
    "\tsource, target = vector[:num_in], vector[-num_out:]\n",
    "\txminus, x = inject_source(source, w)\n",
    "\txminus, x = forward_prop(xminus, x, w)\n",
    "\tdelta = inject_target(xminus, target, w, case = case)\n",
    "\tdelta = backward_prop(xminus, x, delta, w)\n",
    "\ty = xminus[outputs(w)]\n",
    "\tloss = J(y, target, w, case = case)\n",
    "\t# gradJ has same shape as w\n",
    "\tgradJ = tensor(delta, x)\n",
    "\t# cast loss to array same shape as w\n",
    "\tloss = full(shape(w), loss)\n",
    "\treturn array([loss, gradJ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# update only weights on edges\n",
    "\n",
    "def update_weights(w, gradJ, learning_rate):\n",
    "\tw[edges(w)] -= learning_rate * gradJ[edges(w)]\n",
    "\treturn w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75859b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_training(vector, w, lr, iters, random = 'no', case = 'square'):\n",
    "\tW = initial_weights(w, random = random)\n",
    "\ttrajectory = [ ]\n",
    "\tfor iter in range(iters):\n",
    "\t\tloss, gradJ = sample_weight_gradient(vector, W, case = case)\n",
    "\t\tW = update_weights(W, gradJ, lr)\n",
    "\t\t# recover scalar loss\n",
    "\t\tloss = loss[0,0]\n",
    "\t\tif isclose(0,loss): break\n",
    "\t\ttrajectory.append(loss)\n",
    "\treturn W, trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44018a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = .045\n",
    "iters = 100\n",
    "wstar, trajectory = sample_training(vector, w, lr, iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "for lr in [.025,.035, .045,.047]:\n",
    "\twstar, trajectory = sample_training(vector, w, lr, iters)\n",
    "\tn = len(trajectory)\n",
    "\tlabel = f'{n}, {lr}'\n",
    "\tplot(range(n), trajectory, label = label)\n",
    "\n",
    "title('sample loss function decay')\n",
    "grid()\n",
    "legend()\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_weight_gradient(dataset, w, case = 'square'):\n",
    "\tswg = lambda v: sample_weight_gradient(v, w, case = case)\n",
    "\tweight_gradients = array([ swg(v) for v in dataset ])\n",
    "\treturn mean(weight_gradients, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f772af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_training(dataset, w, lr, epochs, random = 'no', case = 'square'):\n",
    "\tW = initial_weights(w, random = random)\n",
    "\ttrajectory = [ ]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tloss, gradJ = batch_weight_gradient(dataset, W, case = case)\n",
    "\t\tW = update_weights(W, gradJ, lr)\n",
    "\t\t# recover scalar loss\n",
    "\t\tloss = loss[0,0]\n",
    "\t\tif isclose(0,loss): break\n",
    "\t\ttrajectory.append(loss)\n",
    "\treturn W, trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c95fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sources = array([ [ 1.5, 2.5 ], [1.2,3.1], [7.1,8.2] ])\n",
    "targets = array([ [ 0.427, -0.288 ], [1.1,-.4], [1.2,3.4 ] ])\n",
    "dataset = hstack([sources, targets])\n",
    "\n",
    "epochs = 60\n",
    "\n",
    "for lr in [.05,.06,.07,.08]:\n",
    "\twstar, trajectory = batch_training(dataset, w, lr, epochs)\n",
    "\tn = len(trajectory)\n",
    "\tlabel = f'{n}, {lr}'\n",
    "\tplot(range(n), trajectory, label = label)\n",
    "\n",
    "title('mean loss function decay')\n",
    "grid()\n",
    "legend()\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stochastic_training(dataset, w, lr, epochs, random = 'no', case = 'square'):\n",
    "\tW = initial_weights(w, random = random)\n",
    "\ttrajectory = [ ]\n",
    "\tfor n, epoch in enumerate(range(epochs), start = 1):\n",
    "\t\tlosses = [ ]\n",
    "\t\trng().shuffle(dataset)\n",
    "\t\tfor vector in dataset:\n",
    "\t\t\tloss, gradJ = sample_weight_gradient(vector, W, case = case)\n",
    "\t\t\tW = update_weights(W, gradJ, lr/n)\n",
    "\t\t\t# append scalar loss\n",
    "\t\t\tlosses.append(loss[0,0])\n",
    "\t\tloss = mean(losses)\n",
    "\t\tif isclose(0,loss): break\n",
    "\t\ttrajectory.append(loss)\n",
    "\treturn W, trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf564d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def minibatch_training(dataset, w, lr, s, epochs, random = 'no', case = 'square'):\n",
    "\tW = initial_weights(w, random = random)\n",
    "\ttrajectory = [ ]\n",
    "\tN = len(dataset)\n",
    "\tfor n, epoch in enumerate(range(epochs), start = 1):\n",
    "\t\tlosses = [ ]\n",
    "\t\trng().shuffle(dataset)\n",
    "\t\tminibatches = arange(0, N, s)\n",
    "\t\tfor start in minibatches:\n",
    "\t\t\tend = start + s\n",
    "\t\t\tminibatch = dataset[start: end]\n",
    "\t\t\tloss, gradJ = batch_weight_gradient(minibatch, W, case = case)\n",
    "\t\t\tW = update_weights(W, gradJ, lr/n)\n",
    "\t\t\t# append scalar loss\n",
    "\t\t\tlosses.append(loss[0,0])\n",
    "\t\tloss = mean(losses)\n",
    "\t\tif isclose(0,loss): break\n",
    "\t\ttrajectory.append(loss)\n",
    "\treturn W, trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17807ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = array([\n",
    "[ 0.99335999,  1.        ,  0.        ],\n",
    "[-0.8943543 ,  0.        ,  1.        ],\n",
    "[ 0.87709524,  1.        ,  0.        ],\n",
    "[-0.61427175,  0.        ,  1.        ],\n",
    "[ 0.53202877,  1.        ,  0.        ],\n",
    "[ 1.10156379,  1.        ,  0.        ],\n",
    "[ 0.51760267,  1.        ,  0.        ],\n",
    "[-1.30845517,  0.        ,  1.        ],\n",
    "[ 0.47808674,  1.        ,  0.        ],\n",
    "[-1.13024748,  0.        ,  1.        ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu,  N = 0, 50\n",
    "#  sdev = 0.5, 1.0, 2.0, 10.0\n",
    "sdev = 0.5\n",
    "n, p = 1, 0.5\n",
    "\n",
    "targets = rng().binomial(n,p,N)\n",
    "# source mean equals pm1 according to target=1 or 0\n",
    "sources = rng().normal(mu,sdev,N) + (2*targets - 1)\n",
    "# one-hot encoded targets\n",
    "targets = array([targets, 1-targets]).T\n",
    "\n",
    "dataset = column_stack([sources, targets])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
